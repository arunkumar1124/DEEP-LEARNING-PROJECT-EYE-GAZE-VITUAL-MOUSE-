ğŸ‘ï¸ğŸ–±ï¸ Adaptive Eye-Gaze Virtual Mouse using Deep Learning
A hands-free human-computer interaction system that uses eye movement and facial gestures to control the mouse pointer, enabling accessible and intuitive device control for users with physical limitations.

ğŸ“Œ Project Description
This project introduces a deep learning-based virtual mouse system that leverages MediaPipe, CNNs, and PyAutoGUI to enable cursor control using eye gaze, blinks, and facial gestures. It provides a non-contact, vision-based interface suitable for people with motor impairments and for hands-free applications.

âœ¨ Features
ğŸ” Real-time eye tracking using webcam

ğŸ–±ï¸ Cursor movement with eye direction

ğŸ‘ï¸ Blink-based clicking gestures

ğŸ¤ Optional voice command integration

ğŸ§  Personalized user calibration for better control

ğŸ”„ Multithreaded system for simultaneous gesture and voice input

ğŸ§° Technologies Used
Python

OpenCV

MediaPipe (Face Mesh)

TensorFlow / Keras (for gesture recognition)

PyAutoGUI

SpeechRecognition + pyttsx3 (voice control)

ğŸ§  System Architecture
Eye Movement Detection

Uses MediaPipe FaceMesh to extract facial landmarks.

Tracks movement of eye landmarks and maps them to screen coordinates.

Gesture Recognition

Eye blinks and head gestures (like nods) mapped to mouse click actions.

Mouse Control & Interaction

PyAutoGUI simulates mouse movement and click events.

Optional voice commands trigger complex interactions.

Multithreading

Eye tracking and voice recognition run concurrently using threading.

ğŸ“¦ Installation
bash
Copy
Edit
git clone [https://github.com/your-username/eye-gaze-virtual-mouse.git](https://github.com/arunkumar1124/DEEP-LEARNING-PROJECT-EYE-GAZE-VITUAL-MOUSE-.git)
cd eye-gaze-virtual-mouse
pip install -r requirements.txt
â–¶ï¸ Running the Application
bash
Copy
Edit
python main.py
Make sure your webcam is connected and accessible.

ğŸ“ˆ Results & Performance
Gesture classification accuracy: ~95%

Real-time processing: < 0.5 seconds latency

Cursor tracking smoothness: Highly responsive

ğŸ§ª Future Improvements
Add drag and drop

Expand to AR/VR interactions

Integrate gaze-based keyboard

Use deep CNN for improved gesture classification

ğŸ§‘â€ğŸ’» Authors
Arunkumar M (221501013)

Charu Stash P (221501021)
Department of Artificial Intelligence and Machine Learning
Rajalakshmi Engineering College, Thandalam

ğŸ“œ License
This project is licensed under the MIT License.
